{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целью работы является построение модели, которая могла бы классифицировать комментарии пользователей интернет-магазина \"Викишоп\" на позитивные и негативные.\n",
    "\n",
    "Заказчик предоставил исходные данные, в которых имеется разметка о токсичности комментариев.\n",
    "\n",
    "Качество модели необходимо будет измерять метрикой **F1**, ее результат на тестовой выборке должен быть не меньше `0,75`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание данных:**\n",
    "\n",
    "Данные находятся в файле toxic_comments.csv:\n",
    "\n",
    "- **text** - текст комментария\n",
    "- **toxic** — целевой признак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Импорт-библиотек-и-подготовка-данных\" data-toc-modified-id=\"Импорт-библиотек-и-подготовка-данных-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Импорт библиотек и подготовка данных</a></span></li><li><span><a href=\"#Обучение-моделей\" data-toc-modified-id=\"Обучение-моделей-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF-+-LogisticRegression\" data-toc-modified-id=\"TF-IDF-+-LogisticRegression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>TF-IDF + LogisticRegression</a></span></li><li><span><a href=\"#fastText\" data-toc-modified-id=\"fastText-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>fastText</a></span></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>BERT</a></span></li></ul></li><li><span><a href=\"#Проверка-на-тестовой-выборке\" data-toc-modified-id=\"Проверка-на-тестовой-выборке-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Проверка на тестовой выборке</a></span></li><li><span><a href=\"#Общий-вывод-по-работе\" data-toc-modified-id=\"Общий-вывод-по-работе-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Общий вывод по работе</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (4.61.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем необходимые библиотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import fasttext\n",
    "except:\n",
    "    !pip install fasttext\n",
    "    import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.9/site-packages (3.2.0)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.61.2)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (924 kB)\n",
      "\u001b[K     |████████████████████████████████| 924 kB 58.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.21.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.8)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: confection, thinc, spacy-legacy, pathy, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.17\n",
      "    Uninstalling thinc-8.0.17:\n",
      "      Successfully uninstalled thinc-8.0.17\n",
      "  Attempting uninstall: spacy-legacy\n",
      "    Found existing installation: spacy-legacy 3.0.10\n",
      "    Uninstalling spacy-legacy-3.0.10:\n",
      "      Successfully uninstalled spacy-legacy-3.0.10\n",
      "  Attempting uninstall: pathy\n",
      "    Found existing installation: pathy 0.6.2\n",
      "    Uninstalling pathy-0.6.2:\n",
      "      Successfully uninstalled pathy-0.6.2\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.2.0\n",
      "    Uninstalling spacy-3.2.0:\n",
      "      Successfully uninstalled spacy-3.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.2.0 requires spacy<3.3.0,>=3.2.0, but you have spacy 3.5.2 which is incompatible.\u001b[0m\n",
      "Successfully installed confection-0.0.4 pathy-0.10.1 spacy-3.5.2 spacy-legacy-3.0.12 thinc-8.1.10\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.4)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.61.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.0.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.2.0\n",
      "    Uninstalling en-core-web-sm-3.2.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.2.0\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install torch\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "except:\n",
    "    !pip install transformers\n",
    "    from transformers import TrainingArguments, Trainer\n",
    "    from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# если можно, открываем файл из локального носителя\n",
    "# или с Jupyter Hub\n",
    "try:\n",
    "    df = pd.read_csv('toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "5    \"\\n\\nCongratulations from me as well, use the ...\n",
       "6         COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "7    Your vandalism to the Matt Shirvington article...\n",
       "8    Sorry if the word 'nonsense' was offensive to ...\n",
       "9    alignment on this subject and which are contra...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем 159 тысяч строк, на английском языке. В тексте встречаются знаки препинания, разный регистр букв, а также символы переноса текста (`\\n`). Пропусков не имеется.\n",
    "\n",
    "Явно выражен дисбаланс классов: токсичных комментариев только 10% от всего числа данных. При делении на выборки будем использовать параметр `stratify`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу выделим 20% данных на проведение теста модели, полученной в ходе этой работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['toxic'])\n",
    "train, test = train.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью регулярных выражений очистим текст от всех символов, кроме слов, пробелов и апострофов. Приведем к нижнему регистру:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(x):\n",
    "    \n",
    "    '''Функция, принимает на вход текст, и возвращает текст согласно шаблону, приведенный к нижнему регистру'''\n",
    "    \n",
    "    tx = ' '.join((re.sub(r'[^\\w\\s\\']',' ',x).split())).lower()\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lemma_text'] = train['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['lemma_text'] = test['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         it's been nearly two months and you still have...\n",
       "1         i'm withdrawing my support i do not support wi...\n",
       "2         what is this all about the day before yesterda...\n",
       "3         a demon possessed pedophile pedophile alone wa...\n",
       "4         you are abusing your position as admin to trol...\n",
       "                                ...                        \n",
       "127428    well so far there is no such article you can m...\n",
       "127429    2012 utc you clean up after me when i self nom...\n",
       "127430    question it seems that ip 98 179 149 193 has b...\n",
       "127431    seattle biomed again did you read my message t...\n",
       "127432    i started a discussion thread hi thanks for re...\n",
       "Name: lemma_text, Length: 127433, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['lemma_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Если можно, загружаем файл с лемматизированным текстом\n",
    "# Если файла нет, приведем к лемме (занимает 12 минут)\n",
    "\n",
    "try:\n",
    "    train = pd.read_csv('train_with_lemm.csv', encoding='utf-8')\n",
    "except:\n",
    "    text_raw = train['lemma_text'].values\n",
    "    lemmed_final = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    for text in nlp.pipe(text_raw,  disable=['parser','ner'], n_process=-1):\n",
    "        lemmed_final.append(' '.join([token.lemma_ for token in text]))\n",
    "    train['lemma_text'] = pd.Series(lemmed_final)\n",
    "    train.to_csv('train_with_lemm.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lemm = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом этапе мы рассмотрели исходные данные, очистили текст от символов, привели к нижнему регистру. Также отложили тестовую выборку на конечную оценку качества модели. После лемматизировали тренировочный текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При планировании работы я решил воспользоваться TF-IDF с последующим применением логистической регрессии, моделью fastText (более новая и точная версия библиотеки векторного представления языка чем Word2Vec) и моделью BERT с тонкой настройкой параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT буду обучать на графическом процессоре на удаленном сервере. После загружу модель в тетрадь с кодом настройки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаками сделаем векторы слов и векторы словосочетаний каждого текста. Сначала проведем векторизацию, после с помощью `hstack` соберем признаки. Также при векторизации слов передадим стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 2),\n",
    "    max_features=50000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для промежуточной оценки качества выделим валидационную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    train['lemma_text'], train['toxic'], test_size=0.1, random_state=42, stratify=train['toxic'])\n",
    "features_train, features_valid = features_train.values.astype('str'), features_valid.values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.fit(features_train)\n",
    "train_word_features = word_vectorizer.transform(features_train)\n",
    "valid_word_features = word_vectorizer.transform(features_valid)\n",
    "\n",
    "char_vectorizer.fit(features_train)\n",
    "train_char_features = char_vectorizer.transform(features_train)\n",
    "valid_char_features = char_vectorizer.transform(features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = hstack([train_char_features, train_word_features])\n",
    "valid_features = hstack([valid_char_features, valid_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем модель логистической регрессии\n",
    "clf = LogisticRegression(C=5, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=5, solver=&#x27;sag&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=5, solver=&#x27;sag&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=5, solver='sag')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_features, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# настраиваем отсечку, максимизируя метрику F1\n",
    "\n",
    "predict = clf.predict_proba(valid_features)[:,1]\n",
    "best_thres = 0.01\n",
    "max_f_score = 0\n",
    "for i in range(1,100):\n",
    "    thres = i / 100\n",
    "    var = f1_score(target_valid, list(map(int, predict >= thres)))\n",
    "    if var > max_f_score:\n",
    "        max_f_score = var\n",
    "        best_thres = thres\n",
    "best_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.7940823670531787\n"
     ]
    }
   ],
   "source": [
    "print('F1 = {}'.format(max_f_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохое качество, выдвинутого заказчиком порога мы уже добились. Из минусов данной модели - плохо работает с новыми словами в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошая библиотека, но признаки в нее подаются определнным способом. Подготовим их.\n",
    "\n",
    "Во многих туториалах используют \"сырой\" текст. Поэкспериментировав на наших данных, пришел к выводу, что лемматизированные данные выдают более высокое качество. Их и будем использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>it be be nearly two month and you still have n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I be withdraw my support I do not support wiki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>what be this all about the day before yesterda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>a demon possess pedophile pedophile alone be n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>you be abuse your position as admin to troll a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127428</th>\n",
       "      <td>0</td>\n",
       "      <td>well so far there be no such article you can m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127429</th>\n",
       "      <td>0</td>\n",
       "      <td>2012 utc you clean up after I when I self nomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127430</th>\n",
       "      <td>0</td>\n",
       "      <td>question it seem that ip 98 179 149 193 have b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127431</th>\n",
       "      <td>0</td>\n",
       "      <td>seattle biome again do you read my message to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127432</th>\n",
       "      <td>0</td>\n",
       "      <td>I start a discussion thread hi thank for reply...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127433 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic                                         lemma_text\n",
       "0           0  it be be nearly two month and you still have n...\n",
       "1           0  I be withdraw my support I do not support wiki...\n",
       "2           0  what be this all about the day before yesterda...\n",
       "3           1  a demon possess pedophile pedophile alone be n...\n",
       "4           0  you be abuse your position as admin to troll a...\n",
       "...       ...                                                ...\n",
       "127428      0  well so far there be no such article you can m...\n",
       "127429      0  2012 utc you clean up after I when I self nomi...\n",
       "127430      0  question it seem that ip 98 179 149 193 have b...\n",
       "127431      0  seattle biome again do you read my message to ...\n",
       "127432      0  I start a discussion thread hi thank for reply...\n",
       "\n",
       "[127433 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['toxic','lemma_text']]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для промежуточной оценки качества выделим валидационную выборку.\n",
    "train, valid = train_test_split(\n",
    "    train, test_size=0.1, random_state=42, stratify=train['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73699</th>\n",
       "      <td>0</td>\n",
       "      <td>and 216 37 216 42 warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109288</th>\n",
       "      <td>0</td>\n",
       "      <td>thank I will contact you via your talk page he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73742</th>\n",
       "      <td>0</td>\n",
       "      <td>your word you will retract your comment from m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99434</th>\n",
       "      <td>1</td>\n",
       "      <td>thank you hei it be I the william hope fan gue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14653</th>\n",
       "      <td>1</td>\n",
       "      <td>be you still a dirty jew be you accept jesus a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28554</th>\n",
       "      <td>0</td>\n",
       "      <td>post 2001 bike it be unclear whether the gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114388</th>\n",
       "      <td>0</td>\n",
       "      <td>what policy be I not go by be there some rule ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123950</th>\n",
       "      <td>0</td>\n",
       "      <td>dyk do you know be update on 24 february 2007 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0</td>\n",
       "      <td>talkpage please do not edit my talkpage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65205</th>\n",
       "      <td>0</td>\n",
       "      <td>I do not want it to be read at the time I do n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12744 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic                                         lemma_text\n",
       "73699       0                             and 216 37 216 42 warn\n",
       "109288      0  thank I will contact you via your talk page he...\n",
       "73742       0  your word you will retract your comment from m...\n",
       "99434       1  thank you hei it be I the william hope fan gue...\n",
       "14653       1  be you still a dirty jew be you accept jesus a...\n",
       "...       ...                                                ...\n",
       "28554       0  post 2001 bike it be unclear whether the gentl...\n",
       "114388      0  what policy be I not go by be there some rule ...\n",
       "123950      0  dyk do you know be update on 24 february 2007 ...\n",
       "1794        0            talkpage please do not edit my talkpage\n",
       "65205       0  I do not want it to be read at the time I do n...\n",
       "\n",
       "[12744 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим префикс в наш таргет (так требует fastText).\n",
    "Для дальнейшего сохранения в обучающий файл выделим все данные в один столбец.\n",
    "Для этого напишем функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_ft(df):\n",
    "    df = df[['toxic','lemma_text']]\n",
    "    df['true'] = df['toxic']\n",
    "    df['toxic'] = '__class__' + df['toxic'].astype('str') + ' '\n",
    "    df['toxic_lemma_text'] = df['toxic'] + df['lemma_text']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_data_ft(train)\n",
    "valid = create_data_ft(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшего сохранения в обучающий файл выделим все данные в один столбец:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>true</th>\n",
       "      <th>toxic_lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116933</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>do side effect section add</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 do side effect section add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65013</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>photo i d file cucurbita_2011_g1 jpg in that p...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 photo i d file cucurbita_2011_g1 jp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48249</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>agree that the second half be opinion but shou...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 agree that the second half be opini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36054</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>thank thank for all the work you do at wikiped...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 thank thank for all the work you do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93949</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>be there someone who can stop koncorde from ce...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 be there someone who can stop konco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic                                         lemma_text  true   \n",
       "116933  __class__0                          do side effect section add     0  \\\n",
       "65013   __class__0   photo i d file cucurbita_2011_g1 jpg in that p...     0   \n",
       "48249   __class__0   agree that the second half be opinion but shou...     0   \n",
       "36054   __class__0   thank thank for all the work you do at wikiped...     0   \n",
       "93949   __class__0   be there someone who can stop koncorde from ce...     0   \n",
       "\n",
       "                                         toxic_lemma_text  \n",
       "116933              __class__0 do side effect section add  \n",
       "65013   __class__0 photo i d file cucurbita_2011_g1 jp...  \n",
       "48249   __class__0 agree that the second half be opini...  \n",
       "36054   __class__0 thank thank for all the work you do...  \n",
       "93949   __class__0 be there someone who can stop konco...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>true</th>\n",
       "      <th>toxic_lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73699</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>and 216 37 216 42 warn</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 and 216 37 216 42 warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109288</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>thank I will contact you via your talk page he...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 thank I will contact you via your t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73742</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>your word you will retract your comment from m...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 your word you will retract your com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99434</th>\n",
       "      <td>__class__1</td>\n",
       "      <td>thank you hei it be I the william hope fan gue...</td>\n",
       "      <td>1</td>\n",
       "      <td>__class__1 thank you hei it be I the william h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14653</th>\n",
       "      <td>__class__1</td>\n",
       "      <td>be you still a dirty jew be you accept jesus a...</td>\n",
       "      <td>1</td>\n",
       "      <td>__class__1 be you still a dirty jew be you acc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic                                         lemma_text  true   \n",
       "73699   __class__0                              and 216 37 216 42 warn     0  \\\n",
       "109288  __class__0   thank I will contact you via your talk page he...     0   \n",
       "73742   __class__0   your word you will retract your comment from m...     0   \n",
       "99434   __class__1   thank you hei it be I the william hope fan gue...     1   \n",
       "14653   __class__1   be you still a dirty jew be you accept jesus a...     1   \n",
       "\n",
       "                                         toxic_lemma_text  \n",
       "73699                   __class__0 and 216 37 216 42 warn  \n",
       "109288  __class__0 thank I will contact you via your t...  \n",
       "73742   __class__0 your word you will retract your com...  \n",
       "99434   __class__1 thank you hei it be I the william h...  \n",
       "14653   __class__1 be you still a dirty jew be you acc...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем наши данные в файл, чтобы в последствии \"накормить\" ими fastText\n",
    "\n",
    "train.to_csv('fastText.train', header=None, index=False, columns=['toxic_lemma_text'],encoding='utf-8') \n",
    "valid.to_csv('fastText.valid', header=None, index=False, columns=['toxic_lemma_text'],encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 8M words\n",
      "Number of words:  140417\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1267976 lr:  0.000000 avg.loss:  0.093132 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model_ft = fasttext.train_supervised(input='fastText.train', label=\"__class__\", lr=1, epoch=10, loss='softmax', wordNgrams=1, dim=150, thread=2, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.7975206611570247\n"
     ]
    }
   ],
   "source": [
    "# оценим F1 на валидационной выборке.\n",
    "valid['pred'] = valid[\"lemma_text\"].apply(lambda x: model_ft.predict(str(x), k = 1)[0][0])\n",
    "valid['pred'] = valid['pred'].apply(lambda x:' '.join((re.sub(r'[\\D]',' ',x).split()))).astype(int)\n",
    "var = f1_score(valid['true'], valid['pred'])\n",
    "print('F1 = {}'.format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта модель справилась чуть лучше. Однако, главное ее преимущество по сравнению с TF-IDF - лучше справляется с проблемой словарного запаса. Если нет времени и ресурсов на обучение более тяжелых моделий (типа BERT) можно взять ее. Теперь же попробуем добиться более высокого качества при использовании BERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом шаге я приложу код настройки параметров BERT, как я это делал на удаленном сервере. Также прикладываю [ссылку](https://disk.yandex.ru/d/JDzXq9N_Rv9mDg) с данными обученной модели (конфиг, бины). При желании и возможности использовать GPU можно перевести ячейку с кодом из MarkDown и выполнить обучение. Я сделал 8 эпох, так как торопился по времени ко сроку сдаче. Обучение заняло у меня 13 часов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое действие в коде буду сопровождать комментарием, чтобы была ясна логика моих шагов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# выделим необходимые колонки из лемматизированных данных\n",
    "train = train_lemm[['lemma_text','toxic']]\n",
    "\n",
    "# загрузим токенизатор и классификатор из предобученной модели BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "\n",
    "# переключим модель на ресурсы графического процессора\n",
    "model = model.to('cuda')\n",
    "\n",
    "# обозначим таргеты и признаки. Выделим валидационную выборку.\n",
    "X = list(train[\"lemma_text\"].astype(str))\n",
    "y = list(train[\"toxic\"].astype(int))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)\n",
    "\n",
    "# токенизируем наши тексты\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# создадим torch-датасет\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "# напишем функцию вычисления метрик, которую в последующем будем добавлять в Trainer\n",
    "def compute_metrics(p):\n",
    "    print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# определяем аргументы и тренер\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "# начинаем процесс обучения\n",
    "trainer.train()\n",
    "\n",
    ">>> TrainOutput(global_step=81560, training_loss=0.2589868332019561, metrics={'train_runtime': 46937.8872, 'train_samples_per_second': 17.375, 'train_steps_per_second': 1.738, 'total_flos': 2.1458495719784448e+17, 'train_loss': 0.2589868332019561, 'epoch': 8.0})\n",
    "\n",
    "\n",
    "\n",
    "# здесь можно оценить качество на нашей валидационной выборке\n",
    "trainer.evaluate()\n",
    "\n",
    ">>> \n",
    "{'eval_loss': 0.16832734644412994,\n",
    " 'eval_accuracy': 0.95868481971201,\n",
    " 'eval_precision': 0.8102543399273314,\n",
    " 'eval_recall': 0.7749034749034749,\n",
    " 'eval_f1': 0.7921847246891651,\n",
    " 'eval_runtime': 450.3973,\n",
    " 'eval_samples_per_second': 56.588,\n",
    " 'eval_steps_per_second': 7.074,\n",
    " 'epoch': 8.0}\n",
    "\n",
    "\n",
    "# а здесь пример, как нам выводить предсказанные значения нашей модели\n",
    "text = \"That was very good\"\n",
    "# создаем инпут, токенизируя текст, полученный для классификации\n",
    "inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "# в принте пока что получаем логит, поэтому применяем софтмакс для наших значений\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "# переключаем на cpu и переводим в array\n",
    "predictions = predictions.cpu().detach().numpy()\n",
    "predictions\n",
    "\n",
    ">>> \n",
    "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.7286, -3.4281]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
    "tensor([[0.9979, 0.0021]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
    "array([[0.9978854 , 0.00211461]], dtype=float32)\n",
    "\n",
    "\n",
    "\n",
    "# сохраним нашу модель\n",
    "trainer.save_model('CustomModel')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж , также добились F1 равный 79. Читал, что если увеличить количество эпох, то можно было сделать побольше. Рекомендуют 10 - 20 эпох. Однако, время сдачи поджимает.\n",
    "\n",
    "Но на просторах HuggingFace нашел модель, уже обученную для сентимент-анализа. Их там много, но эта встраивается проще всего. Поэтому, можем добавить ее в нашу работу и оценить результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"JungleLee/bert-toxic-comment-classification\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"JungleLee/bert-toxic-comment-classification\", num_labels=2)\n",
    "\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как на наших данных мы ее не обучаем, для оценки промежуточного результата я просто добавлю данные из трейн-выборки. но добавлю рандомные 1000 строк, чтобы сократить время предсказания (у меня без GPU на всех данных заняло бы 13 часов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_bert = train[['lemma_text','true']].sample(1000,random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n",
       "0    893\n",
       "1    107\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentiment_bert['true'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [07:14<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for text in tqdm(train_sentiment_bert['lemma_text']):\n",
    "    pred = pipeline(text[:512])\n",
    "    y_pred.append(1 if pred[0]['label'] == 'toxic' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_bert['pred'] = pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.7857142857142856\n"
     ]
    }
   ],
   "source": [
    "var = f1_score(train_sentiment_bert['true'], train_sentiment_bert['pred'])\n",
    "print('F1 = {}'.format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь получили чуть хуже результат, чем в нашей обученной BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для теста выберу модель fastText, чтобы была возможность запустить ячейку и проверить, действительно ли работа выполнена надлежащим образом. Тем более, все модели прошли порог, выставленный заказчиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если можно, загружаем файл с лемматизированным текстом\n",
    "# Если файла нет, приведем к лемме\n",
    "\n",
    "try:\n",
    "    test = pd.read_csv('test_with_lemm.csv', encoding='utf-8')\n",
    "except:\n",
    "    text_raw = test['lemma_text'].astype(str).values\n",
    "    lemmed_final = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    for text in nlp.pipe(text_raw,  disable=['parser','ner'], n_process=-1):\n",
    "        lemmed_final.append(' '.join([token.lemma_ for token in text]))\n",
    "    test['lemma_text'] = pd.Series(lemmed_final)\n",
    "    test.to_csv('test_with_lemm.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# с помощью ранее написанной функции преобразуем таблицу в формат нашей модели\n",
    "test = create_data_ft(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>true</th>\n",
       "      <th>toxic_lemma_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__class__1</td>\n",
       "      <td>your mum be a big fat whore your mum be a big ...</td>\n",
       "      <td>1</td>\n",
       "      <td>__class__1 your mum be a big fat whore your mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>I have restore the information arbitrarily del...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 I have restore the information arbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>vandalism please stop if you continue to vanda...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 vandalism please stop if you contin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>language some noteworthy and quite convincing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 language some noteworthy and quite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__class__0</td>\n",
       "      <td>please feel free to add anything you feel I 'v...</td>\n",
       "      <td>0</td>\n",
       "      <td>__class__0 please feel free to add anything yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         toxic                                         lemma_text  true   \n",
       "0  __class__1   your mum be a big fat whore your mum be a big ...     1  \\\n",
       "1  __class__0   I have restore the information arbitrarily del...     0   \n",
       "2  __class__0   vandalism please stop if you continue to vanda...     0   \n",
       "3  __class__0   language some noteworthy and quite convincing ...     0   \n",
       "4  __class__0   please feel free to add anything you feel I 'v...     0   \n",
       "\n",
       "                                    toxic_lemma_text  \n",
       "0  __class__1 your mum be a big fat whore your mu...  \n",
       "1  __class__0 I have restore the information arbi...  \n",
       "2  __class__0 vandalism please stop if you contin...  \n",
       "3  __class__0 language some noteworthy and quite ...  \n",
       "4  __class__0 please feel free to add anything yo...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 = 0.7998681173755356\n"
     ]
    }
   ],
   "source": [
    "# оценим F1 на валидационной выборке.\n",
    "test['pred'] = test[\"lemma_text\"].apply(lambda x: model_ft.predict(str(x), k = 1)[0][0])\n",
    "test['pred'] = test['pred'].apply(lambda x:' '.join((re.sub(r'[\\D]',' ',x).split()))).astype(int)\n",
    "var = f1_score(test['true'], test['pred'])\n",
    "print('Test F1 = {}'.format(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод по работе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале нашей работы мы импортировали все библиотеки и изучили исходные данные. Из предобработки этих данных было выполнено:\n",
    "\n",
    "- очистка от ненужных символов с помощью регулярных выражений;\n",
    "\n",
    "- лемматизация с использованием библиотеки spacy;\n",
    "\n",
    "- разделение на тестовую и тренировочную выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее план работы с моделями был определен следующий:\n",
    "\n",
    "- обучить модель на основе использования TF-IDF\n",
    "\n",
    "- обучить модель с помощью одной из библиотек векторного представления языка (Word2Vec, fastText)\n",
    "\n",
    "- обучить модель BERT.\n",
    "\n",
    "\n",
    "На первом шаге векторизировали наши данные. При этом для признаков использовали векторы слов и векторы n-грамм вместе. После обучили модель логистической регрессии.\n",
    "\n",
    "На втором шаге подготовили данные для библиотеки fastText и обучили модель, оценив качество на валидационной выборке.\n",
    "\n",
    "На третьем этапе обучали BERT с использованием предобученной базовой модели. Обучение проходило на сторонней машине, использовался графический процессор, выбрано было 8 эпох. Также попробовали взять уже обученную модель для сентимент-анализа JungleLee, но результат был хуже, чем у нашей модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно отметить качество по заранее определенной метрике(F1).\n",
    "\n",
    "Все наши модели выдали качество на валидационной выборке 0,79.\n",
    "\n",
    "При этом стоит заметить, что TF-IDF будет проигрывать в перспективном использовании, так как не учитывает проблему словарного запаса.\n",
    "\n",
    "Модель BERT - хороший вариант, но тонкая настройка занимает много времени и требует GPU. Показатель качества еще можно поднять, за счет увеличения эпох.\n",
    "\n",
    "fastText - решает проблему времени обучения и словарного запаса. Для себя сделал вывод, что отлично подходит для базовой модели. Если надо быстро нащупать какой-то результат - отличный выбор. Для длительной работы обучал бы BERT.\n",
    "\n",
    "На тестовой выборке был испытан fastText. F1 score = 0,799"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2611,
    "start_time": "2023-05-04T11:20:43.744Z"
   },
   {
    "duration": 1619,
    "start_time": "2023-05-04T11:20:46.358Z"
   },
   {
    "duration": 6687,
    "start_time": "2023-05-04T11:20:47.979Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-04T11:20:54.669Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-04T11:20:54.670Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-04T11:20:54.671Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-04T11:20:54.673Z"
   },
   {
    "duration": 0,
    "start_time": "2023-05-04T11:20:54.674Z"
   },
   {
    "duration": 3923,
    "start_time": "2023-05-04T11:21:43.394Z"
   },
   {
    "duration": 3479,
    "start_time": "2023-05-04T11:23:40.462Z"
   },
   {
    "duration": 49263,
    "start_time": "2023-05-04T11:24:50.593Z"
   },
   {
    "duration": 2325,
    "start_time": "2023-05-04T11:25:39.859Z"
   },
   {
    "duration": 23216,
    "start_time": "2023-05-04T11:25:56.357Z"
   },
   {
    "duration": 6744,
    "start_time": "2023-05-04T11:26:27.423Z"
   },
   {
    "duration": 424,
    "start_time": "2023-05-04T12:11:55.191Z"
   },
   {
    "duration": 6569,
    "start_time": "2023-05-04T12:12:49.257Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-04T12:12:57.962Z"
   },
   {
    "duration": 8419,
    "start_time": "2023-05-04T12:13:00.431Z"
   },
   {
    "duration": 508,
    "start_time": "2023-05-04T12:13:11.492Z"
   },
   {
    "duration": 42,
    "start_time": "2023-05-04T12:13:14.645Z"
   },
   {
    "duration": 43,
    "start_time": "2023-05-04T12:13:25.682Z"
   },
   {
    "duration": 3057,
    "start_time": "2023-05-04T12:13:27.627Z"
   },
   {
    "duration": 3739,
    "start_time": "2023-05-04T12:13:34.908Z"
   },
   {
    "duration": 1215,
    "start_time": "2023-05-04T13:22:53.728Z"
   },
   {
    "duration": 52,
    "start_time": "2023-05-04T13:31:57.428Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-04T13:32:30.940Z"
   },
   {
    "duration": 2920,
    "start_time": "2023-05-04T13:32:43.337Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-04T13:32:48.632Z"
   },
   {
    "duration": 3441,
    "start_time": "2023-05-04T13:32:56.361Z"
   },
   {
    "duration": 34,
    "start_time": "2023-05-04T13:32:59.804Z"
   },
   {
    "duration": 12,
    "start_time": "2023-05-04T13:32:59.840Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-04T13:32:59.854Z"
   },
   {
    "duration": 74,
    "start_time": "2023-05-04T13:33:04.200Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-04T13:33:06.205Z"
   },
   {
    "duration": 2072,
    "start_time": "2023-05-04T13:33:06.854Z"
   },
   {
    "duration": 534,
    "start_time": "2023-05-04T13:33:08.928Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-04T13:33:09.465Z"
   },
   {
    "duration": 1458535,
    "start_time": "2023-05-04T13:33:11.227Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
